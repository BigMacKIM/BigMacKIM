{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "구글드라이브에서 생성한 노트북.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxCp0+HfTZ8SI8OKm8AruS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BigMacKIM/BigMacKIM/blob/main/forTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.w = 1.0\n",
        "    self.b = 1.0\n",
        "  \n",
        "  def forpass(self, x):   # y = wx+b\n",
        "    y_hat = x * self.w + self.b\n",
        "    return y_hat\n",
        "  \n",
        "  def backprop(self, x, err):\n",
        "    w_grad = x *err\n",
        "    b_grad = 1*err\n",
        "    return w_grad, b_grad\n",
        "\n",
        "  def fit(self, x, y, epochs = 100):\n",
        "    for i in range(epochs):\n",
        "      for x_i, y_i in zip(x,y):\n",
        "        y_hat = self.forpass(x_i)\n",
        "        err = -(y_i - y_hat)\n",
        "        w_grad, b_grad = self.backprop(x_i, err)\n",
        "        self.w -= w_grad\n",
        "        self.b -= b_grad\n",
        "\n"
      ],
      "metadata": {
        "id": "EfNTD6g9sZ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticNeuron:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "  def forpass(self,x):\n",
        "    z = np.sum(x * self.w) + self.b\n",
        "    return z;\n",
        "\n",
        "  def backprop(self, x, err):\n",
        "    w_grad = x * err\n",
        "    b_grad = y * err\n",
        "    return w_grad, b_grad;\n",
        "\n",
        "  def fit(self, x, y, epochs = 100):\n",
        "    self.w = np.ones(x.shape[1])\n",
        "    self.b = 0\n",
        "    for i in range(epochs):\n",
        "      for x_i, y_i in zip(x,y):\n",
        "        z = self.forpass(x_i)\n",
        "        a = self.activation(z)\n",
        "        err = -(y_i - a)\n",
        "        w_grad, b_grad = self.backprop(x_i, err)\n",
        "        self.w -= w_grad\n",
        "        self.b -= b_grad\n",
        "\n",
        "  def activation(self,z):\n",
        "    a = 1/ (1 + np.exp(-z))\n",
        "    return a\n",
        "  def predict(self, x):\n",
        "    z = [self.forpass(x_i) for x_i in x]\n",
        "    a = self.activation(np.array(z))\n",
        "    return a > 0.5\n",
        "\n"
      ],
      "metadata": {
        "id": "CfAEiB8UB-da"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}